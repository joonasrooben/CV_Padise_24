{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padise 2024\n",
    "## Masinnägemise töötuba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valige endale huvipakkuv ülesanne, mida lahendada ning probleemide korral guugeldage või küsige juhendajalt või kaasõpilastelt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erinevad ülesanded on erineva raskusastmega, kuid kõik jälgivad sama põhimõtet: \n",
    "\n",
    "1. ülesande lahendab **MUDEL** \n",
    "2. mudeli tööd demonstreerib **veebikaamera liides**\n",
    "\n",
    "Siin on antud üldine veebikaamera liidese tsükkel ning sinna sisse tuleb tavaliselt panna mudel, mis töötleb pilte ja tagastab tulemuse veebikaamera väljundisse i.e ekraanile :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, sys, numpy, os \n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0) #alustab veebikaamera\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read() #loeb sisse kaadri kaamerast\n",
    "    if not ret: #kontrollib et midagi ei läinud valesti\n",
    "        break\n",
    "    ##\n",
    "    ##\n",
    "    ##Siin saab teha kaadritöötlust MUDELI \n",
    "    ##\n",
    "    ##\n",
    "    cv2.imshow('Väljund', frame) #kuvab pildi veebikaamera väljundisse\n",
    "\n",
    "    # Programm lõpeb, kui vajutame 'q' \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Lõpetuseks vabastame kaamera\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. variant : Näotuvastus  \n",
    "(kood on suures osas pärit siit : https://www.geeksforgeeks.org/face-detection-using-python-and-opencv-with-webcam/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Järgnev koodilõik on andmetekogumiseks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "haar_file = './naotuvastus/haarcascade_frontalface_default.xml' #mudeli asukoht\n",
    "datasets = './naotuvastus/datasets' #näopiltide asukoht\n",
    "\n",
    "sub_data = 'Minu Nimi'\t# kaust minu näo piltidest (muuda vastavalt)\n",
    "\n",
    "path = os.path.join(datasets, sub_data) \n",
    "if not os.path.isdir(path): \n",
    "\tos.mkdir(path) \n",
    "\n",
    "# pildi suurus\n",
    "(width, height) = (130, 100)\t \n",
    "\n",
    "#'0' on veebikaamera tüüpiliselt, \n",
    "# kui on probleeme proovi '1'\n",
    "face_cascade = cv2.CascadeClassifier(haar_file) #laeme mudeli \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # The program loops until it has 30 images of the face. \n",
    "count = 1\n",
    "while count < 30: #to-do1: teha see osa ümber nii, et programm koguks etteantud x arvu pilte \n",
    "\t\t\t\t  #to-do2: nõua, et iga järgev pilt oleks eelnevast piisavalt erinev (kasutada nt. eukleidilist kaugust vms.)\n",
    "\t\t\t\t  # või tehakse mingi aja tagant\n",
    "\n",
    "\tret, frame = cap.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "\tfaces = face_cascade.detectMultiScale(gray, 1.3, 4) \n",
    "\tfor (x, y, w, h) in faces: \n",
    "\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2) \n",
    "\t\tface = gray[y:y + h, x:x + w] \n",
    "\t\tface_resize = cv2.resize(face, (width, height)) \n",
    "\t\tcv2.imwrite('% s/% s.png' % (path, count), face_resize) \n",
    "\tcount += 1\n",
    "\t\n",
    "\tcv2.imshow('OpenCV', frame) \n",
    "\tif cv2.waitKey(1) & 0xFF == ord('q'):\t\n",
    "\t\tbreak\n",
    "\n",
    "# Lõpetuseks vabastame kaamera\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "haar_file = './naotuvastus/haarcascade_frontalface_default.xml' #mudeli asukoht\n",
    "datasets = './naotuvastus/datasets' #näopiltide asukoht\n",
    "\n",
    "sub_data = 'Minu Nimi'\t# kaust minu näo piltidest (muuda vastavalt)\n",
    "\n",
    "path = os.path.join(datasets, sub_data) \n",
    "if not os.path.isdir(path): \n",
    "\tos.mkdir(path) \n",
    "\n",
    "# pildi suurus\n",
    "(width, height) = (130, 100)\t \n",
    "\n",
    "#'0' on veebikaamera tüüpiliselt, \n",
    "# kui on probleeme proovi '1'\n",
    "face_cascade = cv2.CascadeClassifier(haar_file) #laeme mudeli \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# See koodijupp teeb veebikaameraga ühe pildi ja kasutab seda hiljem teie ära tundmiseks \n",
    "count = 1\n",
    "#to-do1: teha see osa ümber nii, et programm koguks etteantud x arvu pilte \n",
    "#to-do2: nõua, et iga järgev pilt oleks eelnevast piisavalt erinev (kasutada nt. eukleidilist kaugust vms.)\n",
    "# või tehakse mingi aja tagant\n",
    "\n",
    "ret, frame = cap.read()\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 4) \n",
    "for (x, y, w, h) in faces: \n",
    "\tcv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2) \n",
    "\tface = gray[y:y + h, x:x + w] \n",
    "\tface_resize = cv2.resize(face, (width, height)) \n",
    "\tcv2.imwrite('% s/% s.png' % (path, count), face_resize) \n",
    "\t\n",
    "cv2.imshow('OpenCV', frame) \n",
    "\n",
    "# Lõpetuseks vabastame kaamera\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teine osa treenib mudeli tehtud fotode põhjal ja tuvastab näo ja kuvab kui enesekindel mudel on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognizing Face Please Be in sufficient Lights...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "size = 4\n",
    "\n",
    "# Tuvastame näo\n",
    "print('Recognizing Face Please Be in sufficient Lights...') \n",
    "\n",
    "# Loome piltide järjendi ja vastavate nimede järjendi \n",
    "(images, labels, names, id) = ([], [], {}, 0) \n",
    "for (subdirs, dirs, files) in os.walk(datasets): \n",
    "\tfor subdir in dirs: \n",
    "\t\tnames[id] = subdir \n",
    "\t\tsubjectpath = os.path.join(datasets, subdir) \n",
    "\t\tfor filename in os.listdir(subjectpath): \n",
    "\t\t\tpath = subjectpath + '/' + filename \n",
    "\t\t\tlabel = id\n",
    "\t\t\timages.append(cv2.imread(path, 0)) \n",
    "\t\t\tlabels.append(int(label)) \n",
    "\t\tid += 1\n",
    "(width, height) = (130, 100) \n",
    "\n",
    "# Create a Numpy array from the two lists above \n",
    "(images, labels) = [numpy.array(lis) for lis in [images, labels]] \n",
    "\n",
    "# OpenCV treenib kogutud fotode pealt mudeli\n",
    "# NOTE FOR OpenCV2: remove '.face' \n",
    "model = cv2.face.LBPHFaceRecognizer_create() \n",
    "model.train(images, labels) \n",
    "\n",
    "# Kasutame fisherRecognizer mudelit veebikaameraga \n",
    "face_cascade = cv2.CascadeClassifier(haar_file) \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\tret, frame = cap.read()\n",
    "\tif ret == False:\n",
    "\t\tbreak\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "\tfaces = face_cascade.detectMultiScale(gray, 1.3, 5) \n",
    "\tfor (x, y, w, h) in faces: \n",
    "\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2) \n",
    "\t\tface = gray[y:y + h, x:x + w] \n",
    "\t\tface_resize = cv2.resize(face, (width, height)) \n",
    "\t\t# Proovib nägu tuvastada\n",
    "\t\tprediction = model.predict(face_resize) \n",
    "\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3) \n",
    "\n",
    "\t\tif prediction[1]<500: \n",
    "\n",
    "\t\t\tcv2.putText(frame, '% s - %.0f' %\n",
    "\t\t\t    (names[prediction[0]], prediction[1]), (x-10, y-10), \n",
    "\t\t\t   cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0)) \n",
    "\t\telse: \n",
    "\t\t\tcv2.putText(frame, 'tundmatu',(x-10, y-10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0)) \n",
    "\n",
    "\tcv2.imshow('OpenCV', frame) \n",
    "\n",
    "\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\tbreak\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Proovige tuvastajate erinevates tingimustes (pime, hämar, profiil jne.)\n",
    "2. Proovige lisada ka kaaslaste nägusid ja treenida nende pealt rohkemate nägude tuvastaja\n",
    "3. Proovige lahendada to-do1 ja to-do2 ning vaadake, kas programm läheb paremaks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. variant: Veebikaamerast tahvel (ilmselt ei pane sisse kuna seda on raske tükkideks võtta)\n",
    "\n",
    "1. Lisage värve\n",
    "2. Lisage kasutajaliides :)\n",
    "3. Lisage võimalus pilte salvestada\n",
    "\n",
    "<img src=\"./tahvel.jpg\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "(kood: https://medium.com/@serurays/interactive-canvas-drawing-with-hand-tracking-using-opencv-and-python-204c1ca31522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from Hand_Tracking_Module import HandDetectorMP\n",
    "\n",
    "folder_path = \"Interface_Pics\"\n",
    "img_list = os.listdir(folder_path)\n",
    "\n",
    "overlay_list = []\n",
    "\n",
    "pressed_keys = []\n",
    "\n",
    "for file in img_list:\n",
    "    img = cv2.imread(f'{folder_path}/{file}')\n",
    "    img = cv2.resize(img, (1280, 125))\n",
    "    overlay_list.append(img)\n",
    "\n",
    "header = overlay_list[2]\n",
    "\n",
    "draw_color = (0, 0, 0)\n",
    "\n",
    "xp, yp = 0, 0\n",
    "\n",
    "brush_thickness = 15\n",
    "\n",
    "img_canvas = np.zeros((720, 1280, 3), np.uint8)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "\n",
    "detector = HandDetectorMP(detection_con=0.85)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "\n",
    "    img = detector.find_hands(img)\n",
    "\n",
    "    lm_list = detector.find_position(img, draw=False)\n",
    "\n",
    "    if len(lm_list) != 0:\n",
    "        x1, y1 = lm_list[8][1:]\n",
    "        x2, y2 = lm_list[12][1:]\n",
    "\n",
    "        fingers = detector.fingers_up()\n",
    "        total_fingers = fingers.count(1)\n",
    "\n",
    "        if fingers[1] and fingers[2]:\n",
    "            xp, yp = 0, 0\n",
    "            cv2.rectangle(img, (x1, y1 - 15), (x2, y2 + 25), draw_color, cv2.FILLED)\n",
    "\n",
    "            if y1 < 125:\n",
    "                if 250 < x1 < 350:\n",
    "                    header = overlay_list[4]\n",
    "                    draw_color = (0, 0, 255)\n",
    "                elif 400 < x1 < 550:\n",
    "                    header = overlay_list[0]\n",
    "                    draw_color = (255, 0, 0)\n",
    "                elif 650 < x1 < 850:\n",
    "                    header = overlay_list[3]\n",
    "                    draw_color = (0, 255, 0)\n",
    "                elif 900 < x1 < 1000:\n",
    "                    header = overlay_list[1]\n",
    "                    draw_color = (0, 0, 0)\n",
    "\n",
    "        elif fingers[1]:\n",
    "            cv2.circle(img, (x1, y1), 15, draw_color, cv2.FILLED)\n",
    "\n",
    "            if xp == 0 and yp == 0:\n",
    "                xp, yp = x1, y1\n",
    "\n",
    "            cv2.line(img, (xp, yp), (x1, y1), draw_color, brush_thickness)\n",
    "            cv2.line(img_canvas, (xp, yp), (x1, y1), draw_color, brush_thickness)\n",
    "\n",
    "            xp, yp = x1, y1\n",
    "\n",
    "    img_gray = cv2.cvtColor(img_canvas, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_inv = cv2.threshold(img_gray, 50, 255, cv2.THRESH_BINARY_INV)\n",
    "    img_inv = cv2.cvtColor(img_inv, cv2.COLOR_GRAY2BGR)\n",
    "    img = cv2.bitwise_and(img, img_inv)\n",
    "    img = cv2.bitwise_or(img, img_canvas)\n",
    "\n",
    "    img[0:125, 0:1280] = header\n",
    "\n",
    "    cv2.imshow(\"Canvas\", img)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('c'):\n",
    "        img_canvas = np.zeros((720, 1280, 3), np.uint8)\n",
    "    elif key == ord('+'):\n",
    "        brush_thickness += 1\n",
    "    elif key == ord('-'):\n",
    "        brush_thickness = max(1, brush_thickness - 1)\n",
    "    elif 48 <= key <= 57:\n",
    "        pressed_keys.append(key - 48)\n",
    "\n",
    "        if len(pressed_keys) == 9:\n",
    "            blue = int(''.join(map(str, pressed_keys[:3])))\n",
    "            green = int(''.join(map(str, pressed_keys[4:7])))\n",
    "            red = int(''.join(map(str, pressed_keys[7:10])))\n",
    "\n",
    "            draw_color = (blue, green, red)\n",
    "\n",
    "            print(pressed_keys)\n",
    "\n",
    "            pressed_keys = []\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. variant: Dinomäng käežestidega\n",
    "\n",
    "kood:(https://levelup.gitconnected.com/playing-chromes-dinosaur-game-using-opencv-19b3cf9c3636\n",
    "\n",
    "https://github.com/harshilp24/handdino-opencv)\n",
    "\n",
    "todo:mix it up with the mediapipe lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open Camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while capture.isOpened():\n",
    "\n",
    "    # Capture frames from the camera\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # Get hand data from the rectangle sub window\n",
    "    cv2.rectangle(frame, (100, 100), (300, 300), (0, 255, 0), 0)\n",
    "    crop_image = frame[100:300, 100:300]\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    blur = cv2.GaussianBlur(crop_image, (3, 3), 0)\n",
    "\n",
    "    # Change color-space from BGR -> HSV\n",
    "    hsv = cv2.cvtColor(blur, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a binary image with where white will be skin colors and rest is black\n",
    "    mask2 = cv2.inRange(hsv, np.array([2, 0, 0]), np.array([20, 255, 255]))\n",
    "\n",
    "    # Kernel for morphological transformation\n",
    "    kernel = np.ones((5, 5))\n",
    "\n",
    "    # Apply morphological transformations to filter out the background noise\n",
    "    dilation = cv2.dilate(mask2, kernel, iterations=1)\n",
    "    erosion = cv2.erode(dilation, kernel, iterations=1)\n",
    "\n",
    "    # Apply Gaussian Blur and Threshold\n",
    "    filtered = cv2.GaussianBlur(erosion, (3, 3), 0)\n",
    "    ret, thresh = cv2.threshold(filtered, 127, 255, 0)\n",
    "#####\n",
    "    # Find contours\n",
    "    #image, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours, hierachy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    try:\n",
    "        # Find contour with maximum area\n",
    "        contour = max(contours, key=lambda x: cv2.contourArea(x))\n",
    "\n",
    "        # Create bounding rectangle around the contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(crop_image, (x, y), (x + w, y + h), (0, 0, 255), 0)\n",
    "\n",
    "        # Find convex hull\n",
    "        hull = cv2.convexHull(contour)\n",
    "\n",
    "        # Draw contour\n",
    "        drawing = np.zeros(crop_image.shape, np.uint8)\n",
    "        cv2.drawContours(drawing, [contour], -1, (0, 255, 0), 0)\n",
    "        cv2.drawContours(drawing, [hull], -1, (0, 0, 255), 0)\n",
    "\n",
    "        # Fi convexity defects\n",
    "        hull = cv2.convexHull(contour, returnPoints=False)\n",
    "        defects = cv2.convexityDefects(contour, hull)\n",
    "\n",
    "        # Use cosine rule to find angle of the far point from the start and end point i.e. the convex points (the finger\n",
    "        # tips) for all defects\n",
    "        count_defects = 0\n",
    "\n",
    "        for i in range(defects.shape[0]):\n",
    "            s, e, f, d = defects[i, 0]\n",
    "            start = tuple(contour[s][0])\n",
    "            end = tuple(contour[e][0])\n",
    "            far = tuple(contour[f][0])\n",
    "\n",
    "            a = math.sqrt((end[0] - start[0]) ** 2 + (end[1] - start[1]) ** 2)\n",
    "            b = math.sqrt((far[0] - start[0]) ** 2 + (far[1] - start[1]) ** 2)\n",
    "            c = math.sqrt((end[0] - far[0]) ** 2 + (end[1] - far[1]) ** 2)\n",
    "            angle = (math.acos((b ** 2 + c ** 2 - a ** 2) / (2 * b * c)) * 180) / 3.14\n",
    "\n",
    "            # if angle >= 90 draw a circle at the far point\n",
    "            if angle <= 90:\n",
    "                count_defects += 1\n",
    "                cv2.circle(crop_image, far, 1, [0, 0, 255], -1)\n",
    "\n",
    "            cv2.line(crop_image, start, end, [0, 255, 0], 2)\n",
    "\n",
    "        # Press SPACE if condition is match\n",
    "\n",
    "        if count_defects >= 4:\n",
    "                pyautogui.press('space')\n",
    "                cv2.putText(frame, \"JUMP\", (115, 80), cv2.FONT_HERSHEY_SIMPLEX, 2, 2, 2)\n",
    "\n",
    "        #PLAY RACING GAMES (WASD)\n",
    "        \"\"\"\n",
    "        if count_defects == 1:\n",
    "            pyautogui.press('w')\n",
    "            cv2.putText(frame, \"W\", (115, 80), cv2.FONT_HERSHEY_SIMPLEX, 2, 2, 2)\n",
    "        if count_defects == 2:\n",
    "            pyautogui.press('s')\n",
    "            cv2.putText(frame, \"S\", (115, 80), cv2.FONT_HERSHEY_SIMPLEX, 2, 2, 2)\n",
    "        if count_defects == 3:\n",
    "            pyautogui.press('aw')\n",
    "            cv2.putText(frame, \"aw\", (115, 80), cv2.FONT_HERSHEY_SIMPLEX, 2, 2, 2)\n",
    "        if count_defects == 4:\n",
    "            pyautogui.press('dw')\n",
    "            cv2.putText(frame, \"dw\", (115, 80), cv2.FONT_HERSHEY_SIMPLEX, 2, 2, 2)\n",
    "        if count_defects == 5:\n",
    "            pyautogui.press('s')\n",
    "            cv2.putText(frame, \"s\", (115, 80), cv2.FONT_HERSHEY_SIMPLEX, 2, 2, 2)\n",
    "        \"\"\"\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Show required images\n",
    "    cv2.imshow(\"Gesture\", frame)\n",
    "\n",
    "    # Close the camera if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. variant: Koloriseeri (žesti peale)\n",
    "1. Koloriseerige üks pilt\n",
    "2. Tehke veebikaamera mustvalgeks ja siis koloriseerige\n",
    "3. Lisage mustvalge, koloriseeritud, originaalne režiim žesti peale\n",
    "\n",
    "<img src=\"./bw-to-color.jpg\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "(kood: https://www.geeksforgeeks.org/black-and-white-image-colorization-with-opencv-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "from cv2 import dnn\n",
    "\n",
    "#--------Model file paths--------#\n",
    "proto_file = './colorization/colorization_deploy_v2.prototxt'\n",
    "model_file = './colorization/colorization_release_v2.caffemodel'\n",
    "hull_pts = 'colorization/pts_in_hull.npy'\n",
    "img_path = 'minbw.jpg'\n",
    "#--------------#--------------#\n",
    "\n",
    "#--------Reading the model params--------#\n",
    "net = dnn.readNetFromCaffe(proto_file,model_file)\n",
    "kernel = np.load(hull_pts)\n",
    "#-----------------------------------#---------------------#\n",
    "\n",
    "#-----Reading and preprocessing image--------#\n",
    "img = cv2.imread(img_path)\n",
    "scaled = img.astype(\"float32\") / 255.0\n",
    "lab_img = cv2.cvtColor(scaled, cv2.COLOR_BGR2LAB)\n",
    "#-----------------------------------#---------------------#\n",
    "\n",
    "# add the cluster centers as 1x1 convolutions to the model\n",
    "class8 = net.getLayerId(\"class8_ab\")\n",
    "conv8 = net.getLayerId(\"conv8_313_rh\")\n",
    "pts = kernel.transpose().reshape(2, 313, 1, 1)\n",
    "net.getLayer(class8).blobs = [pts.astype(\"float32\")]\n",
    "net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype=\"float32\")]\n",
    "#-----------------------------------#---------------------#\n",
    "\n",
    "# we'll resize the image for the network\n",
    "resized = cv2.resize(lab_img, (224, 224))\n",
    "# split the L channel\n",
    "L = cv2.split(resized)[0]\n",
    "# mean subtraction\n",
    "L -= 50\n",
    "#-----------------------------------#---------------------#\n",
    "\n",
    "# predicting the ab channels from the input L channel\n",
    "\n",
    "net.setInput(cv2.dnn.blobFromImage(L))\n",
    "ab_channel = net.forward()[0, :, :, :].transpose((1, 2, 0))\n",
    "# resize the predicted 'ab' volume to the same dimensions as our\n",
    "# input image\n",
    "ab_channel = cv2.resize(ab_channel, (img.shape[1], img.shape[0]))\n",
    "\n",
    "\n",
    "# Take the L channel from the image\n",
    "L = cv2.split(lab_img)[0]\n",
    "# Join the L channel with predicted ab channel\n",
    "colorized = np.concatenate((L[:, :, np.newaxis], ab_channel), axis=2)\n",
    "\n",
    "# Then convert the image from Lab to BGR \n",
    "colorized = cv2.cvtColor(colorized, cv2.COLOR_LAB2BGR)\n",
    "colorized = np.clip(colorized, 0, 1)\n",
    "\n",
    "# change the image to 0-255 range and convert it from float32 to int\n",
    "colorized = (255 * colorized).astype(\"uint8\")\n",
    "\n",
    "# Let's resize the images and show them together\n",
    "img = cv2.resize(img,(640,640))\n",
    "colorized = cv2.resize(colorized,(640,640))\n",
    "\n",
    "result = cv2.hconcat([img,colorized])\n",
    "\n",
    "cv2.imshow(\"Grayscale -> Colour\", result)\n",
    "\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Üks võimalik lahendus esimesele kahele punktile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Camera\n",
    "capture = cv2.VideoCapture(0)\n",
    " # add the cluster centers as 1x1 convolutions to the model\n",
    "\n",
    "proto_file = './colorization/colorization_deploy_v2.prototxt'\n",
    "model_file = './colorization/colorization_release_v2.caffemodel'\n",
    "hull_pts = 'colorization/pts_in_hull.npy'\n",
    "img_path = 'minbw.jpg'\n",
    "#--------------#--------------#\n",
    "\n",
    "#--------Reading the model params--------#\n",
    "net = dnn.readNetFromCaffe(proto_file,model_file)\n",
    "kernel = np.load(hull_pts)\n",
    "#-----------------------------------#---------------------#\n",
    "\n",
    "\n",
    "class8 = net.getLayerId(\"class8_ab\")\n",
    "conv8 = net.getLayerId(\"conv8_313_rh\")\n",
    "pts = kernel.transpose().reshape(2, 313, 1, 1)\n",
    "net.getLayer(class8).blobs = [pts.astype(\"float32\")]\n",
    "net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype=\"float32\")]\n",
    "        #-----------------------------------#---------------------#\n",
    "\n",
    "while capture.isOpened():\n",
    "\n",
    "    # Capture frames from the camera\n",
    "    ret, frame = capture.read()\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2BGR)\n",
    "    if ret:\n",
    "        scaled = frame.astype(\"float32\") / 255.0\n",
    "        gray_img = cv2.cvtColor(scaled, cv2.COLOR_BGR2GRAY)\n",
    "        bgr_from_gray = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        lab_img = cv2.cvtColor(bgr_from_gray, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "        #-----------------------------------#---------------------#\n",
    "\n",
    "       \n",
    "        # we'll resize the image for the network\n",
    "        resized = cv2.resize(lab_img, (224, 224))\n",
    "        # split the L channel\n",
    "        L = cv2.split(resized)[0]\n",
    "        # mean subtraction\n",
    "        L -= 50\n",
    "        #-----------------------------------#---------------------#\n",
    "\n",
    "        # predicting the ab channels from the input L channel\n",
    "\n",
    "        net.setInput(cv2.dnn.blobFromImage(L))\n",
    "        ab_channel = net.forward()[0, :, :, :].transpose((1, 2, 0))\n",
    "        # resize the predicted 'ab' volume to the same dimensions as our\n",
    "        # input image\n",
    "        ab_channel = cv2.resize(ab_channel, ((lab_img.shape[1], lab_img.shape[0])))\n",
    "\n",
    "\n",
    "        # Take the L channel from the image\n",
    "        L = cv2.split(lab_img)[0]\n",
    "        # Join the L channel with predicted ab channel\n",
    "        colorized = np.concatenate((L[:, :, np.newaxis], ab_channel), axis=2)\n",
    "\n",
    "        # Then convert the image from Lab to BGR \n",
    "        colorized = cv2.cvtColor(colorized, cv2.COLOR_LAB2BGR)\n",
    "        colorized = np.clip(colorized, 0, 1)\n",
    "\n",
    "        # change the image to 0-255 range and convert it from float32 to int\n",
    "        colorized = (255 * colorized).astype(\"uint8\")\n",
    "\n",
    "        # Let's resize the images and show them together\n",
    "        img = cv2.resize(gray_frame,(640,640))\n",
    "        colorized = cv2.resize(colorized,(640,640))\n",
    "\n",
    "        result = cv2.hconcat([img,colorized])\n",
    "\n",
    "        cv2.imshow(\"Grayscale -> Colour\", result)\n",
    "\n",
    "\n",
    "        # Close the camera if 'q' is pressed\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "padise_laager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
